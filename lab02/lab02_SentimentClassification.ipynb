{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Z9ah23wYq0r"
   },
   "source": [
    "Warmup\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qS8y5_Ewv6n"
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEJM0h_uctzV"
   },
   "outputs": [],
   "source": [
    "#Loading packages\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm \n",
    "import codecs\n",
    "import random\n",
    "\n",
    "\n",
    "#we fix the seeds to get consistent results\n",
    "\n",
    "SEED = 234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FE8B9-L8U0aZ"
   },
   "source": [
    "# Text classification: Sentiment analysis\n",
    "\n",
    "\n",
    "In this notebook we are going to build state-of-the art models for text classification using the example of sentiment analysis. To be more precise, we will build a feed-forward neural network (FFNN) and a convolutional neural network (CNN). We will look into the details of data preparation, functioning of each model and how the performance of those NNs could be measured efficiently. We will start our work using a toy corpus. Further you can extend your knowledge and use a larger dataset.\n",
    "\n",
    "Again we are using [pytorch](https://pytorch.org/), an open source deep learning platform, as our backbone library in the course. \n",
    "\n",
    "Here is our toy training and validation sets. It is good practise to use the validation set (a set representative of the test data). This set is used to tune hyperparameters and choose a configuration for your model to ensure the best performance. \n",
    "\n",
    "Our toy sets are already tokenized and lowercased.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abMSnMwJx8bu"
   },
   "outputs": [],
   "source": [
    " #our toy sentiment analysis corpus\n",
    "\n",
    "train = ['i like his paper !',\n",
    "                'what a well-writen essay !',\n",
    "                'i do not agree with the criticism on this paper',\n",
    "                'well done ! it was an enjoyable reading',\n",
    "                'it was very good . send me a copy please .',\n",
    "                'the argumenation in the paper is very weak',\n",
    "                'poor effort !',\n",
    "                'the methodology could have been more detailed',\n",
    "                'i am not impressed',\n",
    "                'could have done better .']\n",
    "\n",
    "train_labels = [1,1,1,1,1,0,0,0,0,0]\n",
    "\n",
    "\n",
    "valid = ['i like your paper', \n",
    "             'i agree with your results', \n",
    "             'what a success ! a well-writen paper', \n",
    "             'not enough details . very poor', \n",
    "             'i support the criticism',\n",
    "             'could be better']\n",
    "\n",
    "valid_labels = [1,1,1,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0hPlpqVFYpfL"
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "Using the material from the previous lab session define here a method to get a tokenized corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQ5IL1e1GVn6"
   },
   "outputs": [],
   "source": [
    "def get_tokenized_corpus(corpus):\n",
    "\n",
    "  ...\n",
    " \n",
    "  return tokenized_corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cyKCfY6QbO0r"
   },
   "source": [
    "# Word2index dictionary\n",
    "\n",
    "Similar to the way it was done in the previous lab, we define here a method that returns a word to index dictionary. Note that we reserve the 0 index for the *pad* token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9bWQ3zKYKlQU"
   },
   "outputs": [],
   "source": [
    "def get_word2idx(tokenized_corpus):\n",
    "  vocabulary = []\n",
    "  for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "  \n",
    "  \n",
    "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
    "  # we reserve the 0 index for the placeholder token\n",
    "  word2idx['<pad>'] = 0\n",
    " \n",
    "  return word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hve1MProNBvp"
   },
   "source": [
    "# Preparation of inputs\n",
    "\n",
    "The first layer of our FNN will be an embedding (look-up) layer takes as input indexes of tokens (we do not need to one-hot encode our vectors).\n",
    " \n",
    " Q. Why do we need to fix the length of our input vectors (we take the maximum sentence length here) ? This process is referred to as padding. Print the padded input corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J-xp_gKvObfw"
   },
   "outputs": [],
   "source": [
    "def get_model_inputs(tokenized_corpus, word2idx, labels, max_len):\n",
    "\n",
    "  # we index our sentences\n",
    "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
    "  print(vectorized_sents)\n",
    "  \n",
    "  # we create a tensor of a fixed size filled with zeroes for padding\n",
    "\n",
    "  sent_tensor = Variable(torch.zeros((len(vectorized_sents), max_len))).long()\n",
    "  \n",
    "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
    "  \n",
    "  # we fill it with our vectorized sentences \n",
    "  \n",
    "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
    "\n",
    "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
    "\n",
    "  label_tensor = torch.FloatTensor(labels)\n",
    "  \n",
    "  return sent_tensor, label_tensor\n",
    "\n",
    "\n",
    "tokenized_corpus = get_tokenized_corpus(train)\n",
    "\n",
    "sent_lengths = [len(sent) for sent in tokenized_corpus]\n",
    "max_len = np.max(np.array(sent_lengths))\n",
    "\n",
    "word2idx = get_word2idx(tokenized_corpus)\n",
    "\n",
    "train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, train_labels, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGg6CtALe8Va"
   },
   "source": [
    "# Building the Feed-Forward Neural Network\n",
    "\n",
    "We will start by building a very simple feed-forward neural network (FFNN).  \n",
    "\n",
    "Note that our FFNN class is a sub-class of `nn.Module`.  Within the `__init__` we define the layers of the module. Our first layer is an embedding layer (look-up layer). The layer could be initialized with pre-trained embeddings (as we will see at the end of this lab) or could be trained together with other layers. Then we average the embeddings for each sentence (e.g., [Iyyer et al, 2015](http://www.aclweb.org/anthology/P15-1162)). The next layer is a fully connected layer with a *ReLU* activation. The output layer uses no activation.\n",
    "\n",
    "The `forward` method is called when we feed data into our model. Please note that output dimension of each layer is the input dimension of the next one. \n",
    "\n",
    "\n",
    "Q. Recall from the previous lab the functioning of a lookup layer. How does the mapping to the dense representation happen?\n",
    "\n",
    "Q. Implement the averaging of embeddings. Note that in this case of averaging padding is not necessary. Why? Think about other ways to get a representation of sentence where padding would be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LWut1gtXGQN"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_len, num_classes):\n",
    "        \n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        #embedding (lookup layer) layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #hidden layer\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        #activation\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #output layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # we average the embeddings of words in a sentence\n",
    "        \n",
    "        # Q. How to average the embeddings here?\n",
    "        \n",
    "        # averaged = ?\n",
    "        \n",
    "        # (batch size, max sent length, embedding dim) to (batch size, embedding dim)\n",
    "\n",
    "        out = self.fc1(averaged)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PcAI5BtaQMFh"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "In this section we will define the hyperparameters of our model, the loss function, the optimizer and do a number epochs of training over our mini training data. \n",
    "\n",
    "We will use the **Stochastic gradient descent (SGD)** optimizer. Please note such a hyperparameter as the **learning rate**  controls how the weights are adjusted with respect to the loss gradient. The lower the value,  the more fine-grained are weight updates.\n",
    "\n",
    "**Note**: It is a common practise to perform training using mini-batches (sets of training instances seen by the model during weight update step). In this case the epoch loss is defined as the loss averaged across the mini-batches. Here we use a very small dataset and do not define mini-batches.\n",
    "\n",
    "Q. Why is the number of output classes is equal to 1 for binary classification?\n",
    "\n",
    "Q. Try to modify the learning rate of the optimizer in the range from 0.0001 up to 0.5. How the loss will react to these changes?\n",
    "\n",
    "\n",
    "**Note** Learning rate is initially set to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wrtOwTUsyArb"
   },
   "outputs": [],
   "source": [
    "# we will train for N epochs (N times the model will see all the data)\n",
    "epochs=20\n",
    "\n",
    "# the input dimension is the vocabulary size\n",
    "INPUT_DIM = len(word2idx)\n",
    "\n",
    "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# dimensionality of the output of the second hidden layer\n",
    "HIDDEN_DIM = 50\n",
    "\n",
    "#the outut dimension is the number of classes, 1 for binary classification\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "\n",
    "# recall input parameters to our model\n",
    "#embedding_dim, hidden_dim, vocab_size, max_len, num_classes\n",
    "# max_len is the maximum length of the input sentences as we defined during padding\n",
    "\n",
    "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), max_len, OUTPUT_DIM)\n",
    "\n",
    "# we use the stochastic gradient descent (SGD) optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "# we use the Binary cross-entropy loss with sigmoid (applied to logits) \n",
    "#Recall we did not apply any activation to our output layer, we need to make our outputs look like probality.\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "feature = train_sent_tensor\n",
    "target = train_label_tensor\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "  \n",
    "  #to ensure the dropout (exlained later) is \"turned on\" while training\n",
    "  #good practice to include even if do not use here\n",
    "  model.train()\n",
    "  \n",
    "  #we zero the gradients as they are not removed automatically\n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  # queeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1 \n",
    "  predictions = model(feature).squeeze(1)\n",
    "  loss = loss_fn(predictions, target)\n",
    "  #calculate the gradient of each parameter\n",
    "  loss.backward()\n",
    "  #update the parameters using the gradients and optimizer algorithm \n",
    "  optimizer.step()\n",
    "  \n",
    "  epoch_loss = loss.item()\n",
    "  \n",
    "  print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLzHkq6FRULl"
   },
   "source": [
    "# Accuracy\n",
    "\n",
    "In addition to measuring the loss, we can also evaluate the performance of out model. Implement a method to compute accuracy. And display accuracy after each epoch of training in the previous example.\n",
    "\n",
    "**Note** In the case of training with mini-batches the epoch accuracy is defined as the accuracy averaged across the mini-batches. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kBhE6FeRuDu"
   },
   "outputs": [],
   "source": [
    " def accuracy(output, target):\n",
    "    \n",
    "    ...\n",
    " \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2UhMGlMq3Re"
   },
   "source": [
    "# Tuning on the validation set\n",
    "\n",
    "Apply the pre-processing and data preparation procedures to this validation set.\n",
    "\n",
    "Q: Should we re-use the word to index dictionary we created before ? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i9_FERooxUQR"
   },
   "outputs": [],
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pPvJHOKxelR"
   },
   "source": [
    "We will now estimate the loss and the accuracy over the validation set at the end of each epoch.\n",
    " \n",
    "Q. Try to modify the learning rate and the number of epochs now. How will the validation loss and accuracy react to those changes? \n",
    "\n",
    "Typically, training could be stopped as soon as we no longer observe any improvement in the evaluation results (accuracy for our case) over the validation set. When the train accuracy is close to 100% and the validation accuarcy starts to go down we observe the overfitting. \n",
    "\n",
    "\n",
    "**Note** Learning rate is initially set to 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcKPET9BHb-7"
   },
   "outputs": [],
   "source": [
    "# we will train for N epochs (N times the model will see all the data)\n",
    "epochs=20\n",
    "\n",
    "# the input dimension is the vocabulary size\n",
    "INPUT_DIM = len(word2idx)\n",
    "\n",
    "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# dimensionality of the output of the second hidden layer\n",
    "HIDDEN_DIM = 50\n",
    "\n",
    "#the outut dimension is the numeber of classes, 1 for binary classification\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# recall input parameters to our model\n",
    "#embedding_dim, hidden_dim, vocab_size, max_len, num_classes\n",
    "# max_len is the maximum length of the input sentences as we defined during padding\n",
    " \n",
    "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), max_len, OUTPUT_DIM)\n",
    "\n",
    "# we use the stochastic gradient descent (SGD) optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "# we use the Binary cross-entropy loss with sigmoid (applied to logits) \n",
    "#Recall we did not apply any activation to our output layer, we need to make our outputs look like probality.\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "feature_train = train_sent_tensor\n",
    "target_train = train_label_tensor\n",
    "\n",
    "feature_valid = valid_sent_tensor\n",
    "target_valid = valid_label_tensor\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "  \n",
    "  #to ensure the dropout (exlained later) is \"turned on\" while training\n",
    "  #good practice to include even if do not use here\n",
    "  model.train()\n",
    " \n",
    "  #we zero the gradients as they are not removed automatically\n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  # queeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1 \n",
    "  predictions = model(feature_train).squeeze(1)\n",
    "  loss = loss_fn(predictions, target_train)\n",
    "  acc = accuracy(predictions, target_train)\n",
    "  #calculate the gradient of each parameter\n",
    "  loss.backward()\n",
    "  #update the parameters using the gradients and optimizer algorithm \n",
    "  optimizer.step()\n",
    "  \n",
    "  epoch_loss = loss.item()\n",
    "  epoch_acc = acc\n",
    "  \n",
    "  \n",
    "  # this puts the model in \"evaluation mode\" (turns off dropout and batch normalization)\n",
    "  # good practise to include even if we do not use \n",
    "  model.eval()\n",
    "  # we do not compute gradients within this block, we do no training here\n",
    "  with torch.no_grad():\n",
    " \n",
    "    predictions_valid = model(feature_valid).squeeze(1)\n",
    "    loss = loss_fn(predictions_valid, target_valid)\n",
    "    acc = accuracy(predictions_valid, target_valid)\n",
    "    valid_loss = loss.item()\n",
    "    valid_acc = acc\n",
    "  \n",
    "  print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "npzWXeQATPs4"
   },
   "source": [
    "# Testing the model\n",
    "\n",
    "\n",
    "Now let us test our model. We define a small test set. Apply the data preparation procedures to this test set as you did for the validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Cx2eJi1a8R6"
   },
   "outputs": [],
   "source": [
    "test = ['i really like your paper', \n",
    "             'well-done', \n",
    "             'good results for a paper !', \n",
    "             'your effort is poor !', \n",
    "             'not impressed' , \n",
    "             'weak argumentation']\n",
    "\n",
    "test_labels =[1,1,1,0,0,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0T_I3hzxKle"
   },
   "outputs": [],
   "source": [
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPsXq60HUWWq"
   },
   "source": [
    "We can now test our model. Write a method for the computation of F-measure. Compute both F-measure and accuracy for the test set.\n",
    "\n",
    "Q. Are the resulting evaluations different ? How do you interpret those differences? Output predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ixhvaYkW_SfX"
   },
   "outputs": [],
   "source": [
    "def f_measure(output, gold):\n",
    "  \n",
    "  ...\n",
    "  \n",
    "  print(\"Test: Recall: %.2f, Precision: %.2f, F-measure: %.2f\\n\" % (recall, precision, fscore))\n",
    "  \n",
    "\n",
    "\n",
    "# this puts the model in \"evaluation mode\" (turns off dropout and batch normalization)\n",
    "# good practise to include even if we do not use \n",
    "model.eval()\n",
    "\n",
    "feature = test_sent_tensor\n",
    "target = test_label_tensor\n",
    "\n",
    "# we do not compute gradients within this block\n",
    "with torch.no_grad():\n",
    "     \n",
    "    ...\n",
    "    print(f'| Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "    f_measure(predictions, test_labels)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2znwTJeU3UT"
   },
   "source": [
    "# Building the Convolutional Neural Network (CNN)\n",
    "\n",
    "We will implement a model inspired by the state-of-art CNN model as described in \n",
    " [Convolutional Neural Networks for Sentence Classification (Kim, 2014)](https://arxiv.org/abs/1408.5882). \n",
    " \n",
    " We start as for the FFNN model with a look up **embedding layer**. We implement the **convolutional layer** with the help of `nn.Conv2d` and use the *ReLU* activation after it. In the lecture we have seen an example of a 1-dimentional convolution applied to text. The abovementioned paper, being inspired by the convolution for images, applies a 2-dimensional convolution: a (window size, embedding dimension) filter. It cover *n* sequential words, taking embedding dimensions as the width. We then pass the tensors through a max pooling layer. \n",
    " \n",
    " The **max pooling layer ** is typically followed by a **dropout layer**. The latter sets a random set of activations in the max-pooling layer to zero. This prevents the network from learning to rely on specific weights and helps to prevent overfitting. Note that the dropout layer is only used during training, and not during test time.\n",
    " \n",
    " Q. Study the shapes of outputs coming from convolution and max pooling layers. What is the shape of the max pooling layer output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUPETZaOgvgB"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #in_channels -- 1 text channel\n",
    "        #out_channels -- the number of output channels\n",
    "        #kernel_size is (window size x embedding dim)\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size,embedding_dim))\n",
    "        \n",
    "        #the dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "        #the output layer\n",
    "        self.fc = nn.Linear(out_channels, output_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "                \n",
    "        #(batch size, max sent length)\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        #(batch size, max sent length, embedding dim)\n",
    "        \n",
    "        #images have 3 RGB channels \n",
    "        #for the text we add 1 channel\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #(batch size, 1, max sent length, embedding dim)\n",
    "        \n",
    "        feature_maps = self.conv(embedded)\n",
    "        \n",
    "        #Q. what is the shape of the convolution output ?\n",
    "        \n",
    "        \n",
    "        \n",
    "        feature_maps = feature_maps.squeeze(3)\n",
    "        \n",
    "        #Q. why do we reduce 1 dimention here ?\n",
    "                \n",
    "        feature_maps = F.relu(feature_maps)\n",
    "        \n",
    "  \n",
    "        #the max pooling layer\n",
    "        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
    "        \n",
    "        pooled = pooled.squeeze(2)\n",
    "  \n",
    "        #Q. what is the shape of the pooling output ?\n",
    "         \n",
    "        \n",
    "        dropped = self.dropout(pooled)\n",
    " \n",
    "        preds = self.fc(dropped)\n",
    "        \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UZroMvLtFVv"
   },
   "source": [
    "# Training and testing the CNN\n",
    "\n",
    "Here we will define the CNN-specific hyperparameters and perform the network training and testing.\n",
    " \n",
    "Q. Is the performance of CNN different from the performance of FFNN? Output predictions.\n",
    "\n",
    "Q. Is padding necessary for CNN inputs? What is the role of the window size?\n",
    "\n",
    "\n",
    "**Note** Learning rate is initially set to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lq1lcWbqRwNY"
   },
   "outputs": [],
   "source": [
    "epochs=20\n",
    "\n",
    "INPUT_DIM = len(word2idx)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "#the hyperparamerts specific to CNN\n",
    "\n",
    "# we define the number of filters\n",
    "N_OUT_CHANNELS = 100\n",
    "\n",
    "# we define the window size\n",
    "WINDOW_SIZE = 1\n",
    "\n",
    "# we apply the dropout with the probability 0.5\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "feature_train = train_sent_tensor\n",
    "target_train = train_label_tensor\n",
    "\n",
    "feature_valid = valid_sent_tensor\n",
    "target_valid = valid_label_tensor\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "   \n",
    "  model.train()\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  predictions = model(feature_train).squeeze(1)\n",
    "  loss = loss_fn(predictions, target_train)\n",
    "  acc = accuracy(predictions, target_train)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  epoch_loss = loss.item()\n",
    "  epoch_acc = acc\n",
    "  \n",
    "  model.eval()\n",
    "  \n",
    "  with torch.no_grad():\n",
    " \n",
    "    predictions_valid = model(feature_valid).squeeze(1)\n",
    "    loss = loss_fn(predictions_valid, target_valid)\n",
    "    acc = accuracy(predictions_valid, target_valid)\n",
    "    valid_loss = loss.item()\n",
    "    valid_acc = acc\n",
    "  \n",
    "  print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "  \n",
    "model.eval()\n",
    "\n",
    "feature = test_sent_tensor\n",
    "target = test_label_tensor\n",
    "\n",
    "with torch.no_grad():\n",
    " \n",
    "    predictions = model(feature).squeeze(1)\n",
    "    loss = loss_fn(predictions, target)\n",
    "    acc = accuracy(predictions, target)\n",
    "    print(f'| Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "    f_measure(predictions, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f4VCX4XSqvlu"
   },
   "source": [
    "# Initializing CNN with pre-trained representations\n",
    "\n",
    "The work [Convolutional Neural Networks for Sentence Classification (Kim, 2014)](https://arxiv.org/abs/1408.5882) also investigates the exploitation of pre-trained embeddings and demonstrates the efficiency of using them.\n",
    "\n",
    "Try and initialize the CNN embedding layer with the pre-trained GloVe embeddings you used in the previous lab. We encourage you to modify the respective method from the previous lab. Pay particular attention to keeping the correct indexes from the word2ind for the lookup table.\n",
    "\n",
    "Q. What will the embedding for the *pad* token be?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgvTCi68lOGn"
   },
   "outputs": [],
   "source": [
    "\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCsV8mtBg4-Y"
   },
   "outputs": [],
   "source": [
    "wvecs = np.zeros((len(word2idx), 100))\n",
    "\n",
    "with codecs.open('glove.6B.100d.txt', 'r','utf-8') as f: \n",
    " ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WArZ4RyQrMaQ"
   },
   "source": [
    "You can initialize the CNN model embedding layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A62iRrAjrKor"
   },
   "outputs": [],
   "source": [
    "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nK8Sv9dfsG8C"
   },
   "source": [
    "Q. What is the impact of using those pre-trained embeddings on the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Jp5y7vR3Wdt"
   },
   "source": [
    "# Advanced: Experimenting with larger corpora\n",
    "\n",
    "For advanced experiments with a larger dataset, we suggest to use the [IMBD dataset](http://ai.stanford.edu/~amaas/data/sentiment/) of movie reviews available from [`torchtext.datasets`](https://torchtext.readthedocs.io/en/latest/data.html). This module also provides a range of useful functionalities for data preparation: defining a preprocessing pipeline, splitting, batching, padding, iterating through data, loading pre-trained embeddings, building vocabulary, etc. Below we provide an example using the tokenizer as provided by the [spaCy](https://spacy.io/) toolkit.\n",
    " \n",
    "With the batch size provided, `BucketIterator` defines mini-batches by grouping sequences with similar original lengths, so that there is minimal need for padding.  For this bigger dataset, use `.cuda()` on any input batches/tensors, network modules and loss functions to place computations on the GPU.\n",
    "\n",
    "You can start by applying the provided CNN model to this dataset. You may find necessary to permute inputs to make the batch size be the first dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWVu38ePjh-i"
   },
   "outputs": [],
   "source": [
    "# Additional packages\n",
    " \n",
    "from torchtext import data\n",
    "from torch.utils.data import DataLoader\n",
    "import spacy\n",
    "\n",
    "#Fix GPU seeds\n",
    "\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFUUmNs7ONyY"
   },
   "outputs": [],
   "source": [
    "#define our batch size\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#define types of data and their preprocessing\n",
    "\n",
    "text_field = data.Field(tokenize='spacy',lower=True)\n",
    "label_field = data.LabelField(dtype=torch.float)\n",
    "\n",
    "#get pre-defined split\n",
    "train, test_init = datasets.IMDB.splits(text_field, label_field)\n",
    "\n",
    "#define our own validation and test set (initial test set is too large)\n",
    "train, valid_test = train.split(split_ratio=0.9, random_state=random.seed(SEED))\n",
    "valid, test = valid_test.split(split_ratio=0.5, random_state=random.seed(SEED))\n",
    "\n",
    "print(f'Train size: {len(train)}')\n",
    "print(f'Validation size: {len(valid)}')\n",
    "print(f'Test size: {len(test)}')\n",
    "\n",
    "#build vocabulary with maximum size (less frequent words are not considered)\n",
    "# load the pre-trained word embeddings.\n",
    "text_field.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
    "label_field.build_vocab(train)\n",
    "\n",
    "# get iterators over the data\n",
    "# only train is split in mini-batches\n",
    "# validation and test sets are not split (we provide their lengths as the batch size)\n",
    "# place iterators on the GPU\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train, valid, test),\n",
    "                batch_sizes=(BATCH_SIZE, len(valid), len(test)), device=accelerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUPp6ohIVst1"
   },
   "source": [
    "Q. The paper [Convolutional Neural Networks for Sentence Classification (Kim, 2014)](https://arxiv.org/abs/1408.5882) applies 3 convolutional layers in parallel with window sizes [3, 4, 5]. Try to extend our CNN model with 2 more convolution layers and apply these window sizes. Hint: you can use the `nn.ModuleList ` function. Outputs of the pooling layers are concatenated. What will be the effect on the model performance? \n",
    " \n",
    " Below we provide an example of a train method employing mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_EIWqj0bVu8y"
   },
   "outputs": [],
   "source": [
    "def train_model(train_iter, dev_iter, model):\n",
    "        \n",
    "     \n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        model.train()\n",
    "       \n",
    "        #iterate over batches\n",
    "        for batch in train_iter:\n",
    "          \n",
    "          \n",
    "            #place on the GPU          \n",
    "            feature, target = batch.text.cuda(), batch.label.cuda()\n",
    " \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(feature).squeeze(1)\n",
    "             \n",
    "            loss = loss_fn(predictions, target)\n",
    "            acc = binary_accuracy(predictions, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        valid_loss, valid_acc = eval(dev_iter, model)\n",
    "        #average train loss and accuracy over the mini-batches\n",
    "        epoch_loss, epoch_acc = epoch_loss / len(train_iter), epoch_acc / len(train_iter)\n",
    "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHhRfaFazwL-"
   },
   "source": [
    "Q. Pre-processing: experiment with filtering out stop words from input data. What will be the effect on the performance? \n",
    "\n",
    "You may choose to use spaCy to get a list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dAP7GvymwT41"
   },
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "spacy_stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(spacy_stop_words)\n",
    "text_field = data.Field(tokenize='spacy',lower=True,stop_words=spacy_stop_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nK4oRvc9BnUB"
   },
   "source": [
    "Q. Apply a Naive Bayes classifier to the problem. How would it perform for this task? You can use the `sklearn.naive_bayes.MultinomialNB` implementation from the popular [scikit-learn](https://scikit-learn.org) toolkit. Extraction of the data for this purpose could be performed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VO2Fv2z1Dyk2"
   },
   "outputs": [],
   "source": [
    "for example in train:\n",
    "    tweet = example.text\n",
    "    if example.label == 'pos':\n",
    "        label =1\n",
    "    else:\n",
    "        label =0\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lecture3_labwork.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
